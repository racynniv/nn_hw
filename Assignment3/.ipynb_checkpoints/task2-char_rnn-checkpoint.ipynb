{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char-RNN implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. This network is first posted by Andrej Karpathy, you can find out about his original code on https://github.com/karpathy/char-rnn, the original code is written in *lua*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement Char-RNN using Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup\n",
    "In this part, we will read the data of our input text and process the text for later network training. There are two txt files in the data folder, for computing time consideration, we will use tinyshakespeare.txt here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open('data/tinyshakespeare.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "# and let's get a glance of what the text is\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n",
      "First Citi --- characters mapped to int --- > [18 47 56 57 58  1 15 47 58 47]\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "vocab_to_ind = {c: i for i, c in enumerate(vocab)}\n",
    "ind_to_vocab = dict(enumerate(vocab))\n",
    "text_as_int = np.array([vocab_to_ind[c] for c in text], dtype=np.int32)\n",
    "\n",
    "# We mapped the character as indexes from 0 to len(vocab)\n",
    "for char,_ in zip(vocab_to_ind, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), vocab_to_ind[char]))\n",
    "# Show how the first 10 characters from the text are mapped to integers\n",
    "print ('{} --- characters mapped to int --- > {}'.format(text[:10], text_as_int[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating batches\n",
    "Now that we have preprocessed our input data, we then need to partition our data, here we will use mini-batches to train our model, so how will we define our batches?\n",
    "\n",
    "Let's first clarify the concepts of batches:\n",
    "1. **batch_size**: Reviewing batches in CNN, if we have 100 samples and we set batch_size as 10, it means that we will send 10 samples to the network at one time. In RNN, batch_size have the same meaning, it defines how many samples we send to the network at one time.\n",
    "2. **sequence_length**: However, as for RNN, we store memory in our cells, we pass the information through cells, so we have this sequence_length concept, which also called 'steps', it defines how long a sequence is.\n",
    "\n",
    "From above two concepts, we here clarify the meaning of batch_size in RNN. Here, we define the number of sequences in a batch as N and the length of each sequence as M, so batch_size in RNN **still** represent the number of sequences in a batch but the data size of a batch is actually an array of size **[N, M]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish the get_batches() function below to generate mini-batches.\n",
    "\n",
    "Hint: this function defines a generator, use *yield*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(array, n_seqs, n_steps):\n",
    "    '''\n",
    "    Partition data array into mini-batches\n",
    "    input:\n",
    "    array: input data\n",
    "    n_seqs: number of sequences in a batch\n",
    "    n_steps: length of each sequence\n",
    "    output:\n",
    "    x: inputs\n",
    "    y: targets, which is x with one position shift\n",
    "       you can check the following figure to get the sence of what a target looks like\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(array) / batch_size)\n",
    "    # we only keep the full batches and ignore the left.\n",
    "    array = array[:batch_size * n_batches]\n",
    "    array = array.reshape((n_seqs, -1))\n",
    "    \n",
    "    # You should now create a loop to generate batches for inputs and targets\n",
    "    #############################################\n",
    "    #           TODO: YOUR CODE HERE            #\n",
    "    #############################################\n",
    "    \n",
    "    # cite: https://gist.github.com/hackintoshrao/4e23fdd383808228a9f70c8173545d5b\n",
    "    \n",
    "    print(n_batches)\n",
    "    \n",
    "    for n in range(0, array.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = array[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11153\n",
      "x\n",
      " [[18 47 56 57 58  1 15 47 58 47]\n",
      " [ 1 43 52 43 51 63 11  0 37 43]\n",
      " [52 58 43 42  1 60 47 56 58 59]\n",
      " [56 44 53 50 49  6  0 27 52  1]\n",
      " [47 52  1 57 54 47 58 43  1 53]\n",
      " [56 57  6  1 39 52 42  1 57 58]\n",
      " [46 47 51  1 42 53 61 52  1 58]\n",
      " [ 1 40 43 43 52  1 57 47 52 41]\n",
      " [50 58 57  1 51 39 63  1 57 46]\n",
      " [57 47 53 52  1 53 44  1 56 43]]\n",
      "\n",
      "y\n",
      " [[47 56 57 58  1 15 47 58 47 18]\n",
      " [43 52 43 51 63 11  0 37 43  1]\n",
      " [58 43 42  1 60 47 56 58 59 52]\n",
      " [44 53 50 49  6  0 27 52  1 56]\n",
      " [52  1 57 54 47 58 43  1 53 47]\n",
      " [57  6  1 39 52 42  1 57 58 56]\n",
      " [47 51  1 42 53 61 52  1 58 46]\n",
      " [40 43 43 52  1 57 47 52 41  1]\n",
      " [58 57  1 51 39 63  1 57 46 50]\n",
      " [47 53 52  1 53 44  1 56 43 57]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(text_as_int, 10, 10)\n",
    "print(len(text_as_int))\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Char-RNN model\n",
    "In this section, we will build our char-rnn model, it consists of input layer, rnn_cell layer, output layer, loss and optimizer, we will build them one by one.\n",
    "\n",
    "The goal is to predict new text after given prime word, so for our training data, we have to define inputs and targets, here is a figure that explains the structure of the Char-RNN network.\n",
    "\n",
    "![structure](img/charrnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish all TODOs in ecbm4040.CharRNN and the blanks in the following cells.\n",
    "\n",
    "**Note: The training process on following settings of parameters takes about 20 minutes on a GTX 1070 GPU, so you are suggested to use GCP for this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecbm4040.CharRNN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Set sampling as False(default), we can start training the network, we automatically save checkpoints in the folder /checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2         # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps, cell_type='LSTM', rnn_size=rnn_size,\n",
    "               num_layers=num_layers, learning_rate=learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i111_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i111_l256.ckpt\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Set the sampling as True and we can generate new characters one by one. We can use our saved checkpoints to see how the network learned gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i111_l256.ckpt\n",
      "LORD ntesededd hor nstonnt osetot nns tod althorothalssdedtonttteddtes nttosdtonthe hed onsdte alalanet n thilsstonnssddethiltod alstonettthon hisdtt t ntonne t alt h alsd t as th halton t nettorstens tet alssdth n n h tt antostorsttotonned orods nthilansedetennn hate ote art ot ann n tene thin ttthirtet an nse nedtenene hedth atosd nedthat ttentheth heds al alsddet odethat oted tt n th asde teddetessseste nen ns thos odds hilat at asdedt at hones t alarod al osssese osdsds on nthoss t hosedetentottonte n hosse ansessenss nt hilal thosssthosde nedthest n tttth ne teth n ar ods ansenett te n teted annnsthithensse t orttt alarsd h al ttorsdes ore os ntorsttot assesed ode hath netthe torennsdt tthassd horonnesdt ttttte tente hinesttodset astonesstedden hetoste ot on nt at otharsssetodtessddesd hilestesdss nnsdd thes ossttens thinn ort nestortt hasedssdd ann asthothe n an osese hothante attte asdsessdentt honetedtod t thosd ane othennns this otennssdds onsds nnsenettthitedent hothanthis oteses \n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps, rnn_size=rnn_size,\n",
    "               num_layers=num_layers, learning_rate=learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i111_l256.ckpt\n",
      "LORD orth ast hasttodt ttestod otthonnsd anensedennt hon tt oronest hinsddstt n asddt orse hatesedennene honnsse thils ot atod aret nsd his ntonnnentt thansdd he tensd t tos thete annenst alalse ortedet as hin hale todtet oded ttedesd halth hot h tt alatte ased oth hisden onttot n te osenet har nns horonns ator onedtethareddent or asthasttosssensdedthet tonnne ot nttet hantt h osesdde nnthilsenn ant ose ant thint onsten n he har neddtore tone tessse te tharorsesensestottortteton tensedsesthinetentostethorth nne thos thortennt otose tone t tedd tor ttods oretede ane asddt arsthases ototenseddtontos tess tott thet nnss has ans ttedt one nsdt ttothenn tot ore t h th hen odtthil annsdd h h at nnnn thotosddtosedd otodtese hilarostedtont h asdtethironenns anst h tet ath odse hate n n h oddse n n t ttthethonsss oth odsennnete h hinss todtotonst hittetont hasdd hor than h thed te tt or hont h nnnnssent his ans n assess honnte as t osse honnnenssdd teds otor tettethonen onedse h al t ns tesdtot atth\n"
     ]
    }
   ],
   "source": [
    "# choose a checkpoint other than the final one and see the results. It could be nasty, don't worry!\n",
    "#############################################\n",
    "#           TODO: YOUR CODE HERE            #\n",
    "#############################################\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change another type of RNN cell\n",
    "We are using LSTM cell as the original work, but GRU cell is getting more popular today, let's chage the cell in rnn_cell layer to GRU cell and see how it performs. Your number of step should be the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to change your saved checkpoints' name or they will rewrite the LSTM results that you have already saved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.tools.api.generator.api.nn.rnn_cell' has no attribute 'BasicGRUCell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-afd267faf08e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n\u001b[0;32m----> 9\u001b[0;31m                num_layers, learning_rate)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_as_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Assignment3/ecbm4040/CharRNN.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, batch_size, num_steps, cell_type, rnn_size, num_layers, learning_rate, grad_clip, train_keep_prob, sampling)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Assignment3/ecbm4040/CharRNN.py\u001b[0m in \u001b[0;36mrnn_layer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcell_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GRU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropoutWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_keep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.tools.api.generator.api.nn.rnn_cell' has no attribute 'BasicGRUCell'"
     ]
    }
   ],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Compare your result of two networks that you built and the reasons that caused the difference. (It is a qualitative comparison, it should be based on the specific model that you build.)\n",
    "2. Discuss the difference between LSTM cells and GRU cells, what are the pros and cons of using GRU cells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "**Fill in here.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
