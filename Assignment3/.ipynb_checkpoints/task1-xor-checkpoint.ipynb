{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:232: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n",
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1, Part 1: Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider a simple RNN network shown in the following figure, where _wx, wh, b1, w, b2_ are the scalar parameters of the network. The loss function is the **mean squared error (MSE)**. Given input _(x1, x2) = (1, 0)_, ground truth _(g1, g2) = (1, 1), h0 = 0, (wx, wh, b1, w, b2) = (1, 1, 1, 1, 1)_, compute _(dwx, dwh, db1, dw, db2)_, which are the gradients of loss with repect to 5 parameters _(wx, wh, b1, w, b2)_.\n",
    "\n",
    "![bptt](./img/bptt2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "\n",
    "Answer the above question. \n",
    "\n",
    "**Forward**\n",
    "Evaluating the forward direction first, the equation for $h_1$ is given as $h_1 = \\sigma(w_x x * x_1 + w_h * h_0 + b_1)$. When substituting in the corresponding values, this equation becomes $h_1 = \\sigma(1*1+1*0+1) = \\sigma(2) = .881$. Evaluating $y_1$, the equation is $y_1 = \\sigma(w*h_1+b_2) = \\sigma(1*.881+1) = .868$. For $h_2$ the equation is $h_2 = \\sigma(w_x * x + w_h * h + b_1) = \\sigma(1*0 + 1*.881 + 1) = \\sigma(1.881) = .868$. For $y_2$ the equation is $y_2 = \\sigma(w*h_2 + b_1) = \\sigma(1*.868 + 1) = \\sigma(1.868) = .866$\n",
    "\n",
    "**Derivatives**\n",
    "The derivatives were calculated outside of this script. For use in this derivation, the derivative of a sigmoid is $\\sigma' = \\sigma (1-\\sigma)$. The loss of this function is $L = \\frac{(g_1-y_1)^2+(g_2-y_2)^2}{2}$. The derivatives $dL$ are \n",
    "$\\frac{dL}{dw_x} = (g_1-y_1)y_1(1-y_1)w*h_1(1-h_1)x_1 + (g_2-y_2)y_2(1-y_2)w*h_2(1-h_2)(x_2+w_h*h_1(1-h_1)x_1)$\n",
    "\n",
    "$\\frac{dL}{dw_h} = (g_1-y_1)y_1(1-y_1)w*h_1(1-h_1)h_0 + (g_2-y_2)y_2(1-y_2)w*h_2(1-h_2)(h_1+w_h*h_1(1-h_1)h_0)$\n",
    "\n",
    "$\\frac{dL}{dw} = (g_1-y_1)y_1(1-y_1)h_1+(g_2-y_2)y_2(1-y_2)h_2$\n",
    "\n",
    "$\\frac{dL}{db_2}=(g_1-y_1)y_1(1-y_1) + (g_2-y_2)y_2(1-y_2)$\n",
    "\n",
    "$\\frac{dL}{db_1}= (g_1-y_1)y_1(1-y_1)w*h_1(1-h_1) + (g_2-y_2)y_2(1-y_2)w*h_2(1-h_2)(1+w_h*h_1(1-h_1))$\n",
    "\n",
    "By substituting in the values from above and the givens, these equations become\n",
    "$\\frac{dL}{dw_x} = (1-.868).868(1-.868)1*.881(1-.881)1 + (1-.866).866(1-.866)1*.868(1-.868)(0+1*.881(1-.881).881) = .00175$\n",
    "\n",
    "$\\frac{dL}{dw_h} = (1-.868).868(1-.868)1*.881(1-.881)0 + (1-.866).866(1-.866)1*.868(1-.868)(.881+1*.881(1-.881)*0) = .00157$\n",
    "\n",
    "$\\frac{dL}{dw} = (1-.868).868(1-.868)*.881+(1-.866).866(1-.866).868 = .0268$\n",
    "\n",
    "$\\frac{dL}{db_2} = (1-.868).868(1-.868)+(1-.866).866(1-.866) = .0307$\n",
    "\n",
    "$\\frac{dL}{db_1} = (1-.868).868(1-.868)1*.881(1-.881) + (1-.866).866(1-.866)1*.868(1-.868)(1+1*.881(1-.881)) = .00355$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1, Part 2: Use tensorflow modules to create XOR network\n",
    "\n",
    "In this part, you need to build and train an XOR network that can learn the XOR function. It is a very simple implementation of RNN and will give you an idea how RNN is built and how to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR network\n",
    "\n",
    "XOR network can learn the XOR $\\oplus$ function\n",
    "\n",
    "As shown in the figure below, and for instance, if input $(x0, x1, x2)$=(1,0,0), then output $(y1, y2, y3)$=(1,1,1). That is, $y_n = x_0\\oplus x_1 \\oplus ... \\oplus x_{n-1}$\n",
    "\n",
    "![xor_net](./img/xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data set\n",
    "This function provides you the way to generate the data which is required for the training process. You should utilize it when building your training function for the GRU. Please read the source code for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecbm4040.xor.utils import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network using a Tensorlow GRUCell\n",
    "This section shows an example how to build a RNN network using an GRU cell. GRU cell is an inbuilt class in tensorflow which implements the real behavior of the GRU neuron. \n",
    "\n",
    "Reference: \n",
    "1. [TensorFlow GRU cell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/GRUCell)\n",
    "2. [Understanding GRU networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Input shape: (num_samples, seq_length, input_dimension)\n",
    "# Output shape: (num_samples, output_ground_truth), and output_ground_truth is 0/1.\n",
    "input_data = tf.placeholder(tf.float32, shape=[None,None,1])\n",
    "output_data = tf.placeholder(tf.int64, shape=[None,None])\n",
    "\n",
    "# define GRU cell\n",
    "num_units = 64\n",
    "cell = GRUCell(num_units)\n",
    "\n",
    "# create GRU network: you can also choose other modules provided by tensorflow, like static_rnn etc.\n",
    "hidden, _ = tf.nn.dynamic_rnn(cell, input_data, dtype=tf.float32)\n",
    "\n",
    "# generate output from the hidden information\n",
    "output_shape = 2\n",
    "out = tf.layers.dense(hidden, output_shape)\n",
    "pred = tf.argmax(out, axis=2)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=output_data,logits=out))\n",
    "\n",
    "# optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "correct_num = tf.equal(output_data,pred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_num,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "<span style='color:red'>TODO:</span> \n",
    "1. Build your training funciton for RNN; \n",
    "2. Plot the cost during the traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cite: http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 1, Part 3 :  Build your own GRUCell\n",
    "In this part, you need to build your own GRU cell to achieve the GRU functionality. \n",
    "\n",
    "<span style=\"color:red\">TODO:</span> \n",
    "1. Finish class **MyGRUCell** in ecbm4040/xor/rnn.py;\n",
    "2. Write the training function for your RNN;\n",
    "3. Plot the cost during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.xor.rnn import MyGRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recreate xor netowrk with your own GRU cell\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Input shape: (num_samples,seq_length,input_dimension)\n",
    "#Output shape: (num_samples, output_ground_truth), and output_ground_truth is 0/1. \n",
    "input_data = tf.placeholder(tf.float32,shape=[None,None,1])\n",
    "output_data = tf.placeholder(tf.int64,shape=[None,None])\n",
    "\n",
    "# recreate xor netowrk with your own GRU cell\n",
    "num_units = 64\n",
    "cell = MyGRUCell(num_units)\n",
    "\n",
    "# create GRU network: you can also choose other modules provided by tensorflow, like static_rnn etc.\n",
    "hidden, _ = tf.nn.dynamic_rnn(cell,input_data,dtype=tf.float32)\n",
    "\n",
    "# generate output from the hidden information\n",
    "output_shape = 2\n",
    "out = tf.layers.dense(hidden, output_shape)\n",
    "pred = tf.argmax(out,axis=2)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=output_data,logits=out))\n",
    "# optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "# accuracy\n",
    "correct = tf.equal(output_data,pred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TRAINING AND PLOTTING CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
